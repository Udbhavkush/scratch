{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c434fb",
   "metadata": {},
   "source": [
    "In this notebook we are taking neural network approach to bigrams instead of the count approach.\\\n",
    "We will take the input and output the probability distribution of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491c854",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ab24a1",
   "metadata": {},
   "source": [
    "We create a training dataset of the bigrams that we did in the last nbotebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2749dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47afc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d7d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stoi and itos\n",
    "stoi = {}\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {c: i+1 for i, c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a726e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# creating training set\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e70e1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49796051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb6c608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we cannot feed the numbers as it is. We have to convert them into vectors with uniform length\n",
    "# we will use one-hot encoding\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "# in the one_hot function there is no parameter for dtype. We will always get int. So, we have to explicitly cast it to float\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c870de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2015],\n",
       "        [-0.2023],\n",
       "        [-0.0802],\n",
       "        [-0.0802],\n",
       "        [-0.0148]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing the first neuron\n",
    "W = torch.randn((27, 1)) # randn gives the random numbers from the normal function. Size is 27 as input size is 27.\n",
    "\n",
    "xenc @ W # its shape would 5 X 1 (basic matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69292c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will make 27 neurons now\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)\n",
    "(xenc @ W).shape\n",
    "# (5, 27) @ (27, 27) -> (5, 27) => this tells us the firing rate of neurons on those five inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532bae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2129)\n",
      "tensor(-0.2129)\n"
     ]
    }
   ],
   "source": [
    "print((xenc @ W)[2, 20])\n",
    "# this is the firing rate of the 20th neuron on the 2nd example or input\n",
    "print(sum(xenc[2] * W[:, 20])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d99e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are just keeping this as the neural network. Just one layer with 27 neurons with no non linearity. Just the linear output\n",
    "print((xenc @ W).shape)\n",
    "# this is the output of the neural network. How do we interpret them. We would want something like that of a probability. But the numbers we are getting are not all between 0 and 1\n",
    "# We can interpret them as something like log(count) count of bigrams basically\n",
    "# so to make more sense of them and make them more readable, we can exponentiate them\n",
    "\n",
    "logits = xenc @ W # log-counts, logits are unnormalized last layer values\n",
    "counts = logits.exp() # equivalent to N (from bigrams). So after exponentiating it, we can interpret values as the count of bigrams.\n",
    "probs = counts / counts.sum(1, keepdim=True)  # we can interpet this as probabilities\n",
    "# calculating counts by exponentiating and finding out the probabilities is actually softmax function.\n",
    "\n",
    "# we are getting these values from a neural network.\n",
    "# so now what is happening is that we have got logits from W and x. We exponentiate them to get something like counts and from that we transformed into something that looks\n",
    "# like probabilities. All of these operations are differentiable and can be back propagated.\n",
    "\n",
    "# we have to find loss now\n",
    "# loss is average of negative log probability\n",
    "# we have ys that is the output label or sort of the ground truth\n",
    "# we have to take avg of probabilities at the ground truth labels to find the loss value\n",
    "# we have 5 inputs and 5 outputs\n",
    "# for 0th input what is the probability to get the ground truth => probs[0, ys[0]] and so on for every index till 5\n",
    "loss = -probs[torch.arange(5), ys].log().mean() # loss is mean of negative log loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52aa6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53414d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None # setting to zero but a more efficient way to do it by setting it None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f895b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bf348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weights\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf7b036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 228146\n"
     ]
    }
   ],
   "source": [
    "# The full summary of this notebook in code\n",
    "\n",
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "nums = xs.nelement()\n",
    "print(\"Number of samples:\", nums)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b1c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 => Loss: 3.758953809738159\n",
      "step 1 => Loss: 3.371100902557373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2 => Loss: 3.154043197631836\n",
      "step 3 => Loss: 3.020373821258545\n",
      "step 4 => Loss: 2.927711248397827\n",
      "step 5 => Loss: 2.8604023456573486\n",
      "step 6 => Loss: 2.8097290992736816\n",
      "step 7 => Loss: 2.7701022624969482\n",
      "step 8 => Loss: 2.7380728721618652\n",
      "step 9 => Loss: 2.711496353149414\n",
      "step 10 => Loss: 2.6890032291412354\n",
      "step 11 => Loss: 2.6696884632110596\n",
      "step 12 => Loss: 2.6529300212860107\n",
      "step 13 => Loss: 2.638277292251587\n",
      "step 14 => Loss: 2.6253879070281982\n",
      "step 15 => Loss: 2.613990545272827\n",
      "step 16 => Loss: 2.603863477706909\n",
      "step 17 => Loss: 2.5948219299316406\n",
      "step 18 => Loss: 2.5867116451263428\n",
      "step 19 => Loss: 2.579403877258301\n",
      "step 20 => Loss: 2.572789192199707\n",
      "step 21 => Loss: 2.5667762756347656\n",
      "step 22 => Loss: 2.5612878799438477\n",
      "step 23 => Loss: 2.5562586784362793\n",
      "step 24 => Loss: 2.551633596420288\n",
      "step 25 => Loss: 2.547365665435791\n",
      "step 26 => Loss: 2.543415069580078\n",
      "step 27 => Loss: 2.539748430252075\n",
      "step 28 => Loss: 2.5363364219665527\n",
      "step 29 => Loss: 2.5331544876098633\n",
      "step 30 => Loss: 2.5301804542541504\n",
      "step 31 => Loss: 2.5273966789245605\n",
      "step 32 => Loss: 2.5247864723205566\n",
      "step 33 => Loss: 2.522334575653076\n",
      "step 34 => Loss: 2.520029067993164\n",
      "step 35 => Loss: 2.517857789993286\n",
      "step 36 => Loss: 2.515810966491699\n",
      "step 37 => Loss: 2.513878107070923\n",
      "step 38 => Loss: 2.512052059173584\n",
      "step 39 => Loss: 2.5103237628936768\n",
      "step 40 => Loss: 2.5086867809295654\n",
      "step 41 => Loss: 2.5071346759796143\n",
      "step 42 => Loss: 2.5056614875793457\n",
      "step 43 => Loss: 2.5042612552642822\n",
      "step 44 => Loss: 2.5029289722442627\n",
      "step 45 => Loss: 2.5016613006591797\n",
      "step 46 => Loss: 2.5004520416259766\n",
      "step 47 => Loss: 2.4992988109588623\n",
      "step 48 => Loss: 2.498197317123413\n",
      "step 49 => Loss: 2.497144937515259\n",
      "step 50 => Loss: 2.496137857437134\n",
      "step 51 => Loss: 2.495173692703247\n",
      "step 52 => Loss: 2.4942493438720703\n",
      "step 53 => Loss: 2.49336314201355\n",
      "step 54 => Loss: 2.4925124645233154\n",
      "step 55 => Loss: 2.4916954040527344\n",
      "step 56 => Loss: 2.4909095764160156\n",
      "step 57 => Loss: 2.4901540279388428\n",
      "step 58 => Loss: 2.4894261360168457\n",
      "step 59 => Loss: 2.488725423812866\n",
      "step 60 => Loss: 2.488049268722534\n",
      "step 61 => Loss: 2.4873974323272705\n",
      "step 62 => Loss: 2.4867682456970215\n",
      "step 63 => Loss: 2.4861602783203125\n",
      "step 64 => Loss: 2.4855728149414062\n",
      "step 65 => Loss: 2.4850049018859863\n",
      "step 66 => Loss: 2.484455108642578\n",
      "step 67 => Loss: 2.4839231967926025\n",
      "step 68 => Loss: 2.483407735824585\n",
      "step 69 => Loss: 2.4829084873199463\n",
      "step 70 => Loss: 2.482424736022949\n",
      "step 71 => Loss: 2.481954574584961\n",
      "step 72 => Loss: 2.481499195098877\n",
      "step 73 => Loss: 2.4810571670532227\n",
      "step 74 => Loss: 2.4806275367736816\n",
      "step 75 => Loss: 2.480210065841675\n",
      "step 76 => Loss: 2.479804515838623\n",
      "step 77 => Loss: 2.479410409927368\n",
      "step 78 => Loss: 2.4790267944335938\n",
      "step 79 => Loss: 2.4786536693573\n",
      "step 80 => Loss: 2.478290557861328\n",
      "step 81 => Loss: 2.4779367446899414\n",
      "step 82 => Loss: 2.477592706680298\n",
      "step 83 => Loss: 2.4772567749023438\n",
      "step 84 => Loss: 2.4769303798675537\n",
      "step 85 => Loss: 2.476611375808716\n",
      "step 86 => Loss: 2.4763011932373047\n",
      "step 87 => Loss: 2.4759981632232666\n",
      "step 88 => Loss: 2.4757025241851807\n",
      "step 89 => Loss: 2.4754140377044678\n",
      "step 90 => Loss: 2.475132703781128\n",
      "step 91 => Loss: 2.474858045578003\n",
      "step 92 => Loss: 2.4745898246765137\n",
      "step 93 => Loss: 2.474327802658081\n",
      "step 94 => Loss: 2.474071741104126\n",
      "step 95 => Loss: 2.4738216400146484\n",
      "step 96 => Loss: 2.4735772609710693\n",
      "step 97 => Loss: 2.4733381271362305\n",
      "step 98 => Loss: 2.473104476928711\n",
      "step 99 => Loss: 2.4728763103485107\n",
      "step 100 => Loss: 2.4726529121398926\n",
      "step 101 => Loss: 2.4724340438842773\n",
      "step 102 => Loss: 2.4722204208374023\n",
      "step 103 => Loss: 2.472011089324951\n",
      "step 104 => Loss: 2.471806049346924\n",
      "step 105 => Loss: 2.4716053009033203\n",
      "step 106 => Loss: 2.471409320831299\n",
      "step 107 => Loss: 2.4712164402008057\n",
      "step 108 => Loss: 2.4710280895233154\n",
      "step 109 => Loss: 2.470843553543091\n",
      "step 110 => Loss: 2.4706625938415527\n",
      "step 111 => Loss: 2.4704854488372803\n",
      "step 112 => Loss: 2.4703118801116943\n",
      "step 113 => Loss: 2.4701411724090576\n",
      "step 114 => Loss: 2.4699742794036865\n",
      "step 115 => Loss: 2.4698104858398438\n",
      "step 116 => Loss: 2.4696500301361084\n",
      "step 117 => Loss: 2.469492197036743\n",
      "step 118 => Loss: 2.4693377017974854\n",
      "step 119 => Loss: 2.4691858291625977\n",
      "step 120 => Loss: 2.469036817550659\n",
      "step 121 => Loss: 2.468891143798828\n",
      "step 122 => Loss: 2.468747615814209\n",
      "step 123 => Loss: 2.46860671043396\n",
      "step 124 => Loss: 2.468468427658081\n",
      "step 125 => Loss: 2.468332529067993\n",
      "step 126 => Loss: 2.4681990146636963\n",
      "step 127 => Loss: 2.4680681228637695\n",
      "step 128 => Loss: 2.4679391384124756\n",
      "step 129 => Loss: 2.4678127765655518\n",
      "step 130 => Loss: 2.46768856048584\n",
      "step 131 => Loss: 2.4675662517547607\n",
      "step 132 => Loss: 2.4674463272094727\n",
      "step 133 => Loss: 2.4673285484313965\n",
      "step 134 => Loss: 2.467211961746216\n",
      "step 135 => Loss: 2.467097759246826\n",
      "step 136 => Loss: 2.4669857025146484\n",
      "step 137 => Loss: 2.4668753147125244\n",
      "step 138 => Loss: 2.466766595840454\n",
      "step 139 => Loss: 2.4666597843170166\n",
      "step 140 => Loss: 2.466554880142212\n",
      "step 141 => Loss: 2.4664509296417236\n",
      "step 142 => Loss: 2.4663493633270264\n",
      "step 143 => Loss: 2.4662492275238037\n",
      "step 144 => Loss: 2.4661505222320557\n",
      "step 145 => Loss: 2.4660532474517822\n",
      "step 146 => Loss: 2.4659576416015625\n",
      "step 147 => Loss: 2.4658632278442383\n",
      "step 148 => Loss: 2.465770721435547\n",
      "step 149 => Loss: 2.465679407119751\n",
      "step 150 => Loss: 2.4655892848968506\n",
      "step 151 => Loss: 2.465500593185425\n",
      "step 152 => Loss: 2.4654133319854736\n",
      "step 153 => Loss: 2.465327262878418\n",
      "step 154 => Loss: 2.465242624282837\n",
      "step 155 => Loss: 2.4651591777801514\n",
      "step 156 => Loss: 2.4650766849517822\n",
      "step 157 => Loss: 2.4649956226348877\n",
      "step 158 => Loss: 2.4649155139923096\n",
      "step 159 => Loss: 2.464836359024048\n",
      "step 160 => Loss: 2.46475887298584\n",
      "step 161 => Loss: 2.464682102203369\n",
      "step 162 => Loss: 2.464606285095215\n",
      "step 163 => Loss: 2.4645321369171143\n",
      "step 164 => Loss: 2.4644582271575928\n",
      "step 165 => Loss: 2.464385509490967\n",
      "step 166 => Loss: 2.4643139839172363\n",
      "step 167 => Loss: 2.464243173599243\n",
      "step 168 => Loss: 2.4641733169555664\n",
      "step 169 => Loss: 2.464104652404785\n",
      "step 170 => Loss: 2.4640369415283203\n",
      "step 171 => Loss: 2.4639697074890137\n",
      "step 172 => Loss: 2.4639036655426025\n",
      "step 173 => Loss: 2.4638383388519287\n",
      "step 174 => Loss: 2.4637739658355713\n",
      "step 175 => Loss: 2.463710308074951\n",
      "step 176 => Loss: 2.4636473655700684\n",
      "step 177 => Loss: 2.463585376739502\n",
      "step 178 => Loss: 2.4635238647460938\n",
      "step 179 => Loss: 2.463463544845581\n",
      "step 180 => Loss: 2.4634037017822266\n",
      "step 181 => Loss: 2.4633445739746094\n",
      "step 182 => Loss: 2.4632863998413086\n",
      "step 183 => Loss: 2.463228940963745\n",
      "step 184 => Loss: 2.46317195892334\n",
      "step 185 => Loss: 2.4631154537200928\n",
      "step 186 => Loss: 2.463059902191162\n",
      "step 187 => Loss: 2.4630050659179688\n",
      "step 188 => Loss: 2.4629504680633545\n",
      "step 189 => Loss: 2.4628970623016357\n",
      "step 190 => Loss: 2.462843894958496\n",
      "step 191 => Loss: 2.462791681289673\n",
      "step 192 => Loss: 2.4627397060394287\n",
      "step 193 => Loss: 2.462688684463501\n",
      "step 194 => Loss: 2.4626379013061523\n",
      "step 195 => Loss: 2.462587833404541\n",
      "step 196 => Loss: 2.462538242340088\n",
      "step 197 => Loss: 2.462489128112793\n",
      "step 198 => Loss: 2.4624407291412354\n",
      "step 199 => Loss: 2.462392807006836\n",
      "step 200 => Loss: 2.4623453617095947\n",
      "step 201 => Loss: 2.4622981548309326\n",
      "step 202 => Loss: 2.462252140045166\n",
      "step 203 => Loss: 2.4622061252593994\n",
      "step 204 => Loss: 2.462161064147949\n",
      "step 205 => Loss: 2.462116003036499\n",
      "step 206 => Loss: 2.462071418762207\n",
      "step 207 => Loss: 2.4620275497436523\n",
      "step 208 => Loss: 2.461984395980835\n",
      "step 209 => Loss: 2.4619412422180176\n",
      "step 210 => Loss: 2.4618985652923584\n",
      "step 211 => Loss: 2.4618561267852783\n",
      "step 212 => Loss: 2.4618144035339355\n",
      "step 213 => Loss: 2.461773157119751\n",
      "step 214 => Loss: 2.4617326259613037\n",
      "step 215 => Loss: 2.4616918563842773\n",
      "step 216 => Loss: 2.4616518020629883\n",
      "step 217 => Loss: 2.4616122245788574\n",
      "step 218 => Loss: 2.4615728855133057\n",
      "step 219 => Loss: 2.461534023284912\n",
      "step 220 => Loss: 2.4614953994750977\n",
      "step 221 => Loss: 2.4614572525024414\n",
      "step 222 => Loss: 2.4614193439483643\n",
      "step 223 => Loss: 2.4613819122314453\n",
      "step 224 => Loss: 2.4613451957702637\n",
      "step 225 => Loss: 2.461308240890503\n",
      "step 226 => Loss: 2.4612717628479004\n",
      "step 227 => Loss: 2.461236000061035\n",
      "step 228 => Loss: 2.46120023727417\n",
      "step 229 => Loss: 2.461164951324463\n",
      "step 230 => Loss: 2.461129903793335\n",
      "step 231 => Loss: 2.461095094680786\n",
      "step 232 => Loss: 2.4610607624053955\n",
      "step 233 => Loss: 2.461026668548584\n",
      "step 234 => Loss: 2.4609928131103516\n",
      "step 235 => Loss: 2.4609594345092773\n",
      "step 236 => Loss: 2.4609262943267822\n",
      "step 237 => Loss: 2.460893392562866\n",
      "step 238 => Loss: 2.4608612060546875\n",
      "step 239 => Loss: 2.4608287811279297\n",
      "step 240 => Loss: 2.460797071456909\n",
      "step 241 => Loss: 2.4607648849487305\n",
      "step 242 => Loss: 2.460733652114868\n",
      "step 243 => Loss: 2.460702419281006\n",
      "step 244 => Loss: 2.460671901702881\n",
      "step 245 => Loss: 2.460641384124756\n",
      "step 246 => Loss: 2.460610866546631\n",
      "step 247 => Loss: 2.460580825805664\n",
      "step 248 => Loss: 2.4605510234832764\n",
      "step 249 => Loss: 2.4605214595794678\n",
      "step 250 => Loss: 2.4604921340942383\n",
      "step 251 => Loss: 2.460463047027588\n",
      "step 252 => Loss: 2.4604344367980957\n",
      "step 253 => Loss: 2.4604058265686035\n",
      "step 254 => Loss: 2.4603774547576904\n",
      "step 255 => Loss: 2.4603490829467773\n",
      "step 256 => Loss: 2.4603214263916016\n",
      "step 257 => Loss: 2.460293769836426\n",
      "step 258 => Loss: 2.460266351699829\n",
      "step 259 => Loss: 2.4602391719818115\n",
      "step 260 => Loss: 2.460212230682373\n",
      "step 261 => Loss: 2.4601852893829346\n",
      "step 262 => Loss: 2.4601590633392334\n",
      "step 263 => Loss: 2.460132360458374\n",
      "step 264 => Loss: 2.460106372833252\n",
      "step 265 => Loss: 2.460080862045288\n",
      "step 266 => Loss: 2.460055112838745\n",
      "step 267 => Loss: 2.460029363632202\n",
      "step 268 => Loss: 2.4600043296813965\n",
      "step 269 => Loss: 2.4599790573120117\n",
      "step 270 => Loss: 2.459954261779785\n",
      "step 271 => Loss: 2.4599297046661377\n",
      "step 272 => Loss: 2.4599053859710693\n",
      "step 273 => Loss: 2.459881067276001\n",
      "step 274 => Loss: 2.4598567485809326\n",
      "step 275 => Loss: 2.4598326683044434\n",
      "step 276 => Loss: 2.459808826446533\n",
      "step 277 => Loss: 2.4597856998443604\n",
      "step 278 => Loss: 2.4597620964050293\n",
      "step 279 => Loss: 2.4597389698028564\n",
      "step 280 => Loss: 2.4597158432006836\n",
      "step 281 => Loss: 2.45969295501709\n",
      "step 282 => Loss: 2.459670305252075\n",
      "step 283 => Loss: 2.4596478939056396\n",
      "step 284 => Loss: 2.459625720977783\n",
      "step 285 => Loss: 2.4596030712127686\n",
      "step 286 => Loss: 2.459581136703491\n",
      "step 287 => Loss: 2.459559440612793\n",
      "step 288 => Loss: 2.4595375061035156\n",
      "step 289 => Loss: 2.4595158100128174\n",
      "step 290 => Loss: 2.4594945907592773\n",
      "step 291 => Loss: 2.4594736099243164\n",
      "step 292 => Loss: 2.4594523906707764\n",
      "step 293 => Loss: 2.4594311714172363\n",
      "step 294 => Loss: 2.4594104290008545\n",
      "step 295 => Loss: 2.45939040184021\n",
      "step 296 => Loss: 2.459369421005249\n",
      "step 297 => Loss: 2.4593491554260254\n",
      "step 298 => Loss: 2.4593288898468018\n",
      "step 299 => Loss: 2.4593088626861572\n",
      "step 300 => Loss: 2.4592888355255127\n",
      "step 301 => Loss: 2.4592692852020264\n",
      "step 302 => Loss: 2.459249496459961\n",
      "step 303 => Loss: 2.4592299461364746\n",
      "step 304 => Loss: 2.4592106342315674\n",
      "step 305 => Loss: 2.45919132232666\n",
      "step 306 => Loss: 2.459172487258911\n",
      "step 307 => Loss: 2.459153413772583\n",
      "step 308 => Loss: 2.459134817123413\n",
      "step 309 => Loss: 2.459115982055664\n",
      "step 310 => Loss: 2.459097146987915\n",
      "step 311 => Loss: 2.4590790271759033\n",
      "step 312 => Loss: 2.4590606689453125\n",
      "step 313 => Loss: 2.4590423107147217\n",
      "step 314 => Loss: 2.459024429321289\n",
      "step 315 => Loss: 2.4590065479278564\n",
      "step 316 => Loss: 2.458988666534424\n",
      "step 317 => Loss: 2.458970785140991\n",
      "step 318 => Loss: 2.458953380584717\n",
      "step 319 => Loss: 2.458935499191284\n",
      "step 320 => Loss: 2.458918333053589\n",
      "step 321 => Loss: 2.4589009284973145\n",
      "step 322 => Loss: 2.4588840007781982\n",
      "step 323 => Loss: 2.458866834640503\n",
      "step 324 => Loss: 2.4588499069213867\n",
      "step 325 => Loss: 2.4588332176208496\n",
      "step 326 => Loss: 2.4588167667388916\n",
      "step 327 => Loss: 2.4587998390197754\n",
      "step 328 => Loss: 2.4587836265563965\n",
      "step 329 => Loss: 2.4587671756744385\n",
      "step 330 => Loss: 2.4587509632110596\n",
      "step 331 => Loss: 2.4587345123291016\n",
      "step 332 => Loss: 2.4587185382843018\n",
      "step 333 => Loss: 2.458702802658081\n",
      "step 334 => Loss: 2.4586868286132812\n",
      "step 335 => Loss: 2.4586708545684814\n",
      "step 336 => Loss: 2.45865535736084\n",
      "step 337 => Loss: 2.458639621734619\n",
      "step 338 => Loss: 2.4586246013641357\n",
      "step 339 => Loss: 2.458609104156494\n",
      "step 340 => Loss: 2.4585933685302734\n",
      "step 341 => Loss: 2.458578586578369\n",
      "step 342 => Loss: 2.4585633277893066\n",
      "step 343 => Loss: 2.4585485458374023\n",
      "step 344 => Loss: 2.45853328704834\n",
      "step 345 => Loss: 2.4585189819335938\n",
      "step 346 => Loss: 2.4585037231445312\n",
      "step 347 => Loss: 2.4584896564483643\n",
      "step 348 => Loss: 2.45847487449646\n",
      "step 349 => Loss: 2.458460569381714\n",
      "step 350 => Loss: 2.4584460258483887\n",
      "step 351 => Loss: 2.4584317207336426\n",
      "step 352 => Loss: 2.4584174156188965\n",
      "step 353 => Loss: 2.4584035873413086\n",
      "step 354 => Loss: 2.4583892822265625\n",
      "step 355 => Loss: 2.4583754539489746\n",
      "step 356 => Loss: 2.4583616256713867\n",
      "step 357 => Loss: 2.458348035812378\n",
      "step 358 => Loss: 2.458333969116211\n",
      "step 359 => Loss: 2.4583206176757812\n",
      "step 360 => Loss: 2.4583070278167725\n",
      "step 361 => Loss: 2.4582936763763428\n",
      "step 362 => Loss: 2.458280324935913\n",
      "step 363 => Loss: 2.4582669734954834\n",
      "step 364 => Loss: 2.4582536220550537\n",
      "step 365 => Loss: 2.458240509033203\n",
      "step 366 => Loss: 2.4582276344299316\n",
      "step 367 => Loss: 2.458214521408081\n",
      "step 368 => Loss: 2.4582016468048096\n",
      "step 369 => Loss: 2.458188772201538\n",
      "step 370 => Loss: 2.4581758975982666\n",
      "step 371 => Loss: 2.4581634998321533\n",
      "step 372 => Loss: 2.458150625228882\n",
      "step 373 => Loss: 2.4581379890441895\n",
      "step 374 => Loss: 2.458125591278076\n",
      "step 375 => Loss: 2.458113431930542\n",
      "step 376 => Loss: 2.4581010341644287\n",
      "step 377 => Loss: 2.4580883979797363\n",
      "step 378 => Loss: 2.4580767154693604\n",
      "step 379 => Loss: 2.458064317703247\n",
      "step 380 => Loss: 2.458052396774292\n",
      "step 381 => Loss: 2.458040237426758\n",
      "step 382 => Loss: 2.458028554916382\n",
      "step 383 => Loss: 2.4580163955688477\n",
      "step 384 => Loss: 2.458004951477051\n",
      "step 385 => Loss: 2.457993268966675\n",
      "step 386 => Loss: 2.4579813480377197\n",
      "step 387 => Loss: 2.457969903945923\n",
      "step 388 => Loss: 2.457958459854126\n",
      "step 389 => Loss: 2.45794677734375\n",
      "step 390 => Loss: 2.457935094833374\n",
      "step 391 => Loss: 2.4579238891601562\n",
      "step 392 => Loss: 2.4579129219055176\n",
      "step 393 => Loss: 2.4579014778137207\n",
      "step 394 => Loss: 2.457890272140503\n",
      "step 395 => Loss: 2.4578793048858643\n",
      "step 396 => Loss: 2.4578683376312256\n",
      "step 397 => Loss: 2.457857370376587\n",
      "step 398 => Loss: 2.4578464031219482\n",
      "step 399 => Loss: 2.4578356742858887\n",
      "step 400 => Loss: 2.45782470703125\n",
      "step 401 => Loss: 2.4578139781951904\n",
      "step 402 => Loss: 2.457803249359131\n",
      "step 403 => Loss: 2.457792282104492\n",
      "step 404 => Loss: 2.457782030105591\n",
      "step 405 => Loss: 2.4577715396881104\n",
      "step 406 => Loss: 2.45776104927063\n",
      "step 407 => Loss: 2.4577507972717285\n",
      "step 408 => Loss: 2.457740306854248\n",
      "step 409 => Loss: 2.4577298164367676\n",
      "step 410 => Loss: 2.4577198028564453\n",
      "step 411 => Loss: 2.457709550857544\n",
      "step 412 => Loss: 2.4576992988586426\n",
      "step 413 => Loss: 2.4576892852783203\n",
      "step 414 => Loss: 2.457679271697998\n",
      "step 415 => Loss: 2.4576690196990967\n",
      "step 416 => Loss: 2.4576592445373535\n",
      "step 417 => Loss: 2.4576494693756104\n",
      "step 418 => Loss: 2.457639455795288\n",
      "step 419 => Loss: 2.457629919052124\n",
      "step 420 => Loss: 2.457620143890381\n",
      "step 421 => Loss: 2.4576103687286377\n",
      "step 422 => Loss: 2.4576008319854736\n",
      "step 423 => Loss: 2.4575912952423096\n",
      "step 424 => Loss: 2.4575817584991455\n",
      "step 425 => Loss: 2.4575724601745605\n",
      "step 426 => Loss: 2.4575624465942383\n",
      "step 427 => Loss: 2.4575531482696533\n",
      "step 428 => Loss: 2.4575436115264893\n",
      "step 429 => Loss: 2.4575345516204834\n",
      "step 430 => Loss: 2.4575252532958984\n",
      "step 431 => Loss: 2.4575161933898926\n",
      "step 432 => Loss: 2.4575068950653076\n",
      "step 433 => Loss: 2.4574978351593018\n",
      "step 434 => Loss: 2.457488775253296\n",
      "step 435 => Loss: 2.45747971534729\n",
      "step 436 => Loss: 2.457470655441284\n",
      "step 437 => Loss: 2.4574620723724365\n",
      "step 438 => Loss: 2.4574527740478516\n",
      "step 439 => Loss: 2.457443952560425\n",
      "step 440 => Loss: 2.457435369491577\n",
      "step 441 => Loss: 2.4574263095855713\n",
      "step 442 => Loss: 2.4574177265167236\n",
      "step 443 => Loss: 2.457409143447876\n",
      "step 444 => Loss: 2.457400321960449\n",
      "step 445 => Loss: 2.4573915004730225\n",
      "step 446 => Loss: 2.457383155822754\n",
      "step 447 => Loss: 2.4573745727539062\n",
      "step 448 => Loss: 2.4573662281036377\n",
      "step 449 => Loss: 2.45735764503479\n",
      "step 450 => Loss: 2.4573493003845215\n",
      "step 451 => Loss: 2.457340955734253\n",
      "step 452 => Loss: 2.4573326110839844\n",
      "step 453 => Loss: 2.457324504852295\n",
      "step 454 => Loss: 2.4573159217834473\n",
      "step 455 => Loss: 2.457308053970337\n",
      "step 456 => Loss: 2.4572994709014893\n",
      "step 457 => Loss: 2.457291603088379\n",
      "step 458 => Loss: 2.4572837352752686\n",
      "step 459 => Loss: 2.457275390625\n",
      "step 460 => Loss: 2.4572675228118896\n",
      "step 461 => Loss: 2.4572596549987793\n",
      "step 462 => Loss: 2.45725154876709\n",
      "step 463 => Loss: 2.4572436809539795\n",
      "step 464 => Loss: 2.457235813140869\n",
      "step 465 => Loss: 2.4572277069091797\n",
      "step 466 => Loss: 2.4572200775146484\n",
      "step 467 => Loss: 2.457212209701538\n",
      "step 468 => Loss: 2.457204818725586\n",
      "step 469 => Loss: 2.4571967124938965\n",
      "step 470 => Loss: 2.4571890830993652\n",
      "step 471 => Loss: 2.457181692123413\n",
      "step 472 => Loss: 2.4571738243103027\n",
      "step 473 => Loss: 2.4571666717529297\n",
      "step 474 => Loss: 2.4571590423583984\n",
      "step 475 => Loss: 2.4571516513824463\n",
      "step 476 => Loss: 2.457143783569336\n",
      "step 477 => Loss: 2.457136631011963\n",
      "step 478 => Loss: 2.4571292400360107\n",
      "step 479 => Loss: 2.4571218490600586\n",
      "step 480 => Loss: 2.4571146965026855\n",
      "step 481 => Loss: 2.4571075439453125\n",
      "step 482 => Loss: 2.4570999145507812\n",
      "step 483 => Loss: 2.457092761993408\n",
      "step 484 => Loss: 2.457085609436035\n",
      "step 485 => Loss: 2.457078456878662\n",
      "step 486 => Loss: 2.457071304321289\n",
      "step 487 => Loss: 2.457064151763916\n",
      "step 488 => Loss: 2.457056999206543\n",
      "step 489 => Loss: 2.457050323486328\n",
      "step 490 => Loss: 2.457043170928955\n",
      "step 491 => Loss: 2.457036256790161\n",
      "step 492 => Loss: 2.457029104232788\n",
      "step 493 => Loss: 2.4570224285125732\n",
      "step 494 => Loss: 2.4570155143737793\n",
      "step 495 => Loss: 2.4570083618164062\n",
      "step 496 => Loss: 2.4570016860961914\n",
      "step 497 => Loss: 2.4569952487945557\n",
      "step 498 => Loss: 2.4569880962371826\n",
      "step 499 => Loss: 2.4569814205169678\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "steps = 500\n",
    "lr = 50\n",
    "\n",
    "for k in range(steps):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(nums), ys].log().mean()\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    print(f\"step {k} => Loss: {loss.item()}\")\n",
    "    # update weights\n",
    "    W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffc038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezitynn.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n",
      "da.\n",
      "staiyaubrtthrigotai.\n",
      "moliellavo.\n",
      "ke.\n",
      "teda.\n"
     ]
    }
   ],
   "source": [
    "# sampling from this neural network\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0: # we have reached the end of the word\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    \n",
    "\n",
    "# we got the exact same names as we got in the counting bigrams model as both the model exactly represents the same thing hence we got the same loss\n",
    "# bigram counting table model is not scalable as if we want to store more ngrams, it would be too difficult and hence not flexible.\n",
    "# gradient based method is very scalable and flexible that way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
